Below is a clean Sprint 3 markdown, distilled from everything you’ve decided and aligned with the UI concept you shared.
This is not UI code — it is the product + data + logic spine the UI hangs on.

You can hand this directly to the dev team.

⸻


# Veritas — Sprint 3 Build Plan  
## Assessments · Rubrics · Dual Scoring (K–12 + Higher Ed)

**Sprint Goal**  
Enable teachers to create **high-integrity oral assessments** that:
- attach cleanly to Classes
- support visual + oral prompts
- use **dual AI scoring**
- align to **K–12 standards** *and* **Higher Ed expectations**

This sprint delivers the **Assessment Builder backbone**, not polish.

---

## 1. Core Assessment Mental Model (Lock This)

```text
Assessment
  ├── belongs to Class
  ├── has structured segments
  ├── produces student recordings
  ├── scored by two agents
  └── finalized by teacher

Assessments are:
	•	class-scoped
	•	time-bounded
	•	rubric-driven
	•	defensible

⸻

2. Assessment Structure (Composable, Not Flat)

An assessment is composed of segments.

Example:
	•	Visual description
	•	Retell
	•	Reasoned response

Each segment:
	•	has its own prompt
	•	has its own constraints
	•	is scored independently

⸻

3. Core Tables

3.1 assessments

create table assessments (
  id uuid primary key default gen_random_uuid(),
  class_id uuid references classes(id) on delete cascade,
  title text not null,
  subject text,
  target_language text,
  instructions text,
  status text default 'draft', -- draft | live | closed
  created_at timestamptz default now()
);


⸻

3.2 assessment_segments

create table assessment_segments (
  id uuid primary key default gen_random_uuid(),
  assessment_id uuid references assessments(id) on delete cascade,
  segment_type text,  -- visual | retell | open_response
  prompt text,
  time_limit_seconds int,
  view_limit_seconds int,
  order_index int
);

Segments are ordered and independently scored.

⸻

4. Visual Integrity Assets

Table: assessment_assets

create table assessment_assets (
  id uuid primary key default gen_random_uuid(),
  assessment_id uuid references assessments(id) on delete cascade,
  asset_type text, -- image
  asset_url text,
  generation_prompt text,
  created_at timestamptz default now()
);

Rules:
	•	One primary visual per assessment (v1)
	•	Visual anchors reasoning and prevents plagiarism

⸻

5. Question Bank (Segment-Level)

Table: assessment_questions

create table assessment_questions (
  id uuid primary key default gen_random_uuid(),
  segment_id uuid references assessment_segments(id) on delete cascade,
  question_text text,
  question_type text, -- synthesis | recall | interpretation
  order_index int
);

Questions:
	•	are oral
	•	require spontaneous response
	•	cannot be auto-generated by students

⸻

6. Rubrics (This Is the Heart)

Key Decision

Rubrics are segment-specific and dual-scored.

One rubric ≠ one score.

⸻

6.1 rubrics

create table rubrics (
  id uuid primary key default gen_random_uuid(),
  segment_id uuid references assessment_segments(id) on delete cascade,
  rubric_type text, -- reasoning | evidence
  instructions text,
  scale_min int default 1,
  scale_max int default 5
);

Each segment has two rubrics:
	•	Reasoning & synthesis
	•	Evidence & accuracy

⸻

7. Standards Alignment (K–12 + Higher Ed)

Table: rubric_standards

create table rubric_standards (
  id uuid primary key default gen_random_uuid(),
  rubric_id uuid references rubrics(id) on delete cascade,
  framework text, -- CCSS | ACTFL | CEFR | AACU
  standard_code text,
  description text
);

This allows:
	•	standards tagging without hard-coding
	•	K–12 and Higher Ed coexistence
	•	future reporting

⸻

8. Dual Scorer Logic (System-Level)

Concept

Two independent AI agents score each segment.

Segment
  ├── Scorer A → Reasoning
  ├── Scorer B → Evidence
  └── Consensus Layer

Disagreement:
	•	flagged
	•	surfaced to teacher
	•	never silently averaged

⸻

Table: segment_scores

create table segment_scores (
  id uuid primary key default gen_random_uuid(),
  segment_id uuid references assessment_segments(id),
  student_id uuid references students(id),
  scorer_type text, -- reasoning | evidence
  score int,
  justification text,
  created_at timestamptz default now()
);


⸻

9. Integrity Controls (Configurable)

Stored at Assessment Level

create table assessment_integrity (
  assessment_id uuid primary key references assessments(id),
  pause_threshold_seconds numeric,
  tab_switch_monitor boolean,
  shuffle_questions boolean
);

These settings inform scoring, not punish automatically.

⸻

10. Teacher Builder Flow (Functional)

Builder steps:
	1.	Assessment foundation
	2.	Visual asset
	3.	Segment questions
	4.	Rubrics & dual scorer
	5.	Integrity + limits
	6.	Publish to class

Drafts are allowed.
Publishing locks structure.

⸻

11. Sprint 3 Deliverables Checklist

Backend
	•	assessments table
	•	assessment_segments
	•	assessment_assets
	•	assessment_questions
	•	rubrics
	•	rubric_standards
	•	segment_scores
	•	integrity config

Frontend
	•	multi-step assessment builder
	•	segment editor
	•	rubric editor (dual)
	•	publish workflow

Guards
	•	cannot edit structure after publish
	•	segment-level validation
	•	rubric required per segment

⸻

12. What This Unlocks Next

Sprint 4 can cleanly deliver:
	•	student assessment runtime
	•	recording capture
	•	scoring execution
	•	teacher review UI

⸻

13. One-Line Instruction to Dev Team

“Sprint 3 builds the assessment brain — structure first, scoring second, polish later.”

---

addendum

Good catch — and this is an important correction.
This belongs explicitly in Sprint 3, because it defines how teachers actually author assessments.

Below is a clean amendment to Sprint 3 that integrates all three authoring modes without complicating the data model.

You can drop this straight into the Sprint 3 doc as a new section.

⸻


## 0. Assessment Builder — Authoring Modes (Required)

### Core Principle
> **All assessments end up in the same structured model — regardless of how they are created.**

Authoring method affects **how fields are populated**, not how data is stored.

---

## 1. Three Supported Authoring Modes

When a teacher creates a new assessment, they must choose **one** of the following entry paths.

### Mode 1 — Manual Entry (Baseline)

**Who this is for**
- Experienced teachers
- Precise control
- Quick edits

**Behavior**
- Teacher fills each builder step manually:
  - assessment info
  - segments
  - questions
  - rubrics
  - integrity settings

**System role**
- No AI generation
- Validation only

This is the **fallback mode** and must always work.

---

### Mode 2 — Upload & Parse (Document-Ingest)

**Who this is for**
- Teachers with existing materials
- District / Higher Ed adoption
- Rubric reuse

**Accepted uploads**
- PDF
- DOCX
- TXT
- (future: LMS exports)

**System behavior**
1. Teacher uploads file
2. System parses content
3. AI maps content into:
   - assessment title
   - student instructions
   - segment prompts
   - question bank
   - draft rubrics
4. Fields are **pre-filled**, never auto-published

**Teacher must review/edit everything before publish.**

---

### Mode 3 — Prompt-Based Generation (AI Draft)

**Who this is for**
- Teachers starting from scratch
- Rapid iteration
- Exploration

**Teacher provides**
- subject
- grade band (K–12 or Higher Ed)
- target language
- assessment goal
- constraints (time, rigor, standards)

**System behavior**
- AI generates:
  - assessment foundation
  - suggested segments
  - oral questions
  - dual rubrics aligned to level
- Everything is marked **Draft**

Teacher edits → publishes.

---

## 2. UI Flow (Builder Entry Point)

### Route

/assessments/new

### Step 0 — Choose Creation Mode

```text
How would you like to create this assessment?

[ ] Start from scratch
[ ] Upload existing material
[ ] Generate with AI

Once selected:
	•	mode is stored on assessment
	•	builder proceeds to structured steps

⸻

3. Data Model Impact (Minimal, Safe)

Add to assessments

authoring_mode text default 'manual'; 
-- manual | upload | ai

Optional metadata table:

create table assessment_sources (
  assessment_id uuid references assessments(id),
  source_type text,        -- upload | ai
  source_reference text,   -- filename or prompt
  created_at timestamptz default now()
);

Important
	•	Parsed/generated content is written into the same tables:
	•	assessment_segments
	•	assessment_questions
	•	rubrics
	•	No branching schemas.

⸻

4. AI Guardrails (Non-Negotiable)
	•	AI never publishes
	•	AI never bypasses rubrics
	•	AI output is always editable
	•	Teacher is final authority

This preserves defensibility.

⸻

5. Standards-Aware Generation

When using upload or AI generation, system must:
	•	infer grade band
	•	apply correct standards:
	•	K–12 → CCSS / ACTFL / CEFR
	•	Higher Ed → AAC&U / CEFR
	•	tag rubrics accordingly

No standards = no publish.

⸻

6. Why This Matters (Strategic)

This tri-modal builder:
	•	reduces onboarding friction
	•	supports districts with legacy content
	•	showcases AI without surrendering control
	•	avoids “AI black box” perception

Most importantly:

All three paths converge into one defensible assessment structure.

⸻

7. One-Line Instruction to Dev Team

“Sprint 3 supports three creation paths, but only one assessment model.”


